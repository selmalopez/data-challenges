{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to properly prevent overfitting\n",
    "\n",
    "**Objectives:**\n",
    "- Give a `Validation Set` to the model\n",
    "- Use the `Early Stopping` criterion to prevent the Neural network from overfitting\n",
    "- `Regularize` your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "First, let's generate some data thanks to the [`make_blob`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function that we've already used yesterday.\n",
    "\n",
    "❓ **Question** ❓ Generate 2000 samples, with 10 features each. \n",
    "\n",
    "There should be 8 classes of blobs (`centers` argument), with `cluster_std` equal to 7. \n",
    "\n",
    "Plot some dimensions to check your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Use the `to_categorical` function from `tensorflow.keras` to convert `y` to `y_cat` which is the categorical representation of `y` with \"*one-hot encoded*\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part I : Proper cross-validation\n",
    "\n",
    "In a previous challenge, we split the dataset into a train set and a test set at the beginning of the notebook. \n",
    "\n",
    "And then, we started to build different models which were trained on the train set and evaluated on the test set.\n",
    "\n",
    "So, at the end of the day, we used the test set everytime we evaluated our models and different hyperparameters. \n",
    "\n",
    "Therefore, we _used_ the test set to select our best model, which is a sort of ⚠️ `data-leakage` ⚠️.\n",
    "\n",
    "A first good practice is to avoid using `random_state` or any deterministic separation between your train and test set. In that case, your test set will change everytime you re-run your notebook. But this is far from being sufficient.\n",
    "\n",
    "To compare models properly, you have to run a cross-validation, a 10-fold split for instance. Let's see how to do it properly.\n",
    "\n",
    "❓ **Question** ❓ First, write a function that generates a neural network with 3 layers:\n",
    "- a layer with 25 neurons, the `relu` activation function and the appropriate `input_dim`\n",
    "- a layer with 10 neurons and the `relu` activation function.\n",
    "- a last layer which is suited to the problem at hand (multiclass classification)\n",
    "\n",
    "The function should include a compilation method with :\n",
    "- the `categorical_crossentropy` loss, \n",
    "- the `adam` optimizer \n",
    "- and the `accuracy` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will do a proper cross validation.\n",
    "\n",
    "❓ **Question** ❓ Write a loop using the [K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function from Scikit-Learn  (choose 10 splits) to fit your model on the train data, and evaluate it on the test data. Store the results of the evaluation into a `results` variable.\n",
    "\n",
    "Do not forget to standardize your train data before fitting the neural network.\n",
    "\n",
    "Also, 150 epochs should be sufficient for a first approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    # Split the data into train and test\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Initialize the model\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Fit the model on the train data\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Evaluate the model on the test data and append the result in the `results` variable\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Print the mean accuracy, and its standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ You probably encountered one of the main drawbacks of using a proper cross-validation for a Neural Network: **it takes a lot of time** ! Therefore, for the rest of the Deep-Learning module, we will do **only one split**. But remember that this is not entirely correct and, for real-life applications and problems, you are encouraged to use a proper cross-validation technique.\n",
    "\n",
    "❗ **Remark** ❗ In general, what practitioners do, is that they split only once, as you did. And once they get to the end of their optimization, they launch a real cross-validation at 6pm, go home and get the final results on the next day.\n",
    "\n",
    "❓ **Question** ❓ For the rest of the exercise (and of the Deep Learning module), split the dataset into a train set and a test set with a 70/30% training to test data ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Stop the learning process before overfitting\n",
    "\n",
    "Let's first show that if we train the model for too long, too many epochs, it will overfit the training data and will not be good at predicting on the test data.\n",
    "\n",
    "❓ **Question** ❓ To do it, train the same neural network (⚠️ do not forget to re-initialize it ⚠️) with `validation_data=(X_test, y_test)` and 500 epochs. Store the history in a `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate the model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Plot the history of the model with the following function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the number of epochs we chose has a great influence on the final results: \n",
    "\n",
    "* `Unsufficient number of  epochs` $\\rightarrow$ `Underfitting`:\n",
    "    * The algorithm is not optimal as its loss function has not converged yet, \n",
    "    * i.e. it hasn't learnt enough from the training data. \n",
    "* `Too many epochs` $\\rightarrow$ `Overfitting`: \n",
    "    * Our neural network has learnt too much from the training data, even its noisy information... \n",
    "    * and the algorithm does not generalize well on test data.\n",
    "\n",
    "What we want is basically to ***stop the algorithm when the test loss is minimal*** (or when the test accuracy is maximal).\n",
    "\n",
    "Let's introduce the **`Early Stopping`** criterion.\n",
    "\n",
    "The ES criterion is a way to automatically stop the training of the algorithm before the end, before the final number of epochs originally set.\n",
    "\n",
    "❓ How does it work ❓\n",
    "\n",
    "Basically, it uses part of the dataset to check whether the test loss has stopped improving. You cannot use the test data itself to check that, otherwise, it is some kind of data leakage. Instead, we will use a subset of the initial training data, called the **`validation set`**\n",
    "\n",
    "It basically looks like the following 👇\n",
    "\n",
    "<img src=\"validation_set.png\" alt=\"Validation set\" style=\"height:350px;\"/>\n",
    "\n",
    "To split this data, we use the **`validation_split`** keyword which sets the percentage of data from the initial training set used in the validation set.\n",
    "\n",
    "You need to specify it when fitting the model in the `.fit()`. -\n",
    "\n",
    "On top of that, we use the **`callbacks`** keyword to call the Early Stopping criterion at the end of each epoch. You can check additional information in the [documentation](https://www.tensorflow.org/guide/keras/train_and_evaluate)\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Launch the following code, plot the history and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Fit the model on the train data\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=500,\n",
    "                    batch_size=16, \n",
    "                    verbose=1, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ The problem, with this type of approach, is that as soon as the loss of the validation set increases, the model stops. However, as a neural network's convergence is stochastic, it happens that the loss slightly increases before decreasing again. For this reason, the `Early Stopping` criterion has a **`patience`** keyword that `defines how many consecutive epochs without any loss decrease` are allowed before we stop the training procedure.\n",
    "\n",
    "❓ **Question** ❓ Use the Early Stopping criterion with a patience of 30 epochs, plot the results and print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ The model continues to converge even though its loss functions have some consecutive loss increases and decreases. \n",
    "\n",
    "The `patience` number  to select is highly related to the task at hand and there is not any general rule of thumb. \n",
    "\n",
    "❗ **Remark** ❗ If you selected a high patience value, you might face the problem that the loss on the validation set has increased again a lot compared to its lowest value. To that end, the Early Stopping criterion enables you to stop the convergence _and_ **`restore the best weights of the neural network when it had the best score on the validation set`**, thanks to **`restore_best_weights = True`** (that is set to `False` by default).\n",
    "\n",
    "❓ **Question** ❓ Run the model with a Early Stopping criterion that will restore the best weights of the Neural Net, plot the loss and accuracy and print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark 1** ❗ You can look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to play with other parameters\n",
    "\n",
    "❗ **Remark 2** ❗ No longer need to have a look at the epochs as long as the model hits the stopping criterion. So, in the future, you should set a large number of epochs and the early stopping criterion will take care of stopping the training procedure before the model overfits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III : Batch-size & Epochs\n",
    "\n",
    "❓ **Question** ❓ Let's run the previous model with different batch sizes (with the Early Stopping criterion included) and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# RUN THIS CELL (it can take some time...)\n",
    "\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch_size in [1, 4, 32]:\n",
    "    \n",
    "    model = initialize_model()\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split=0.3,\n",
    "                        epochs=500,\n",
    "                        batch_size=batch_size, \n",
    "                        verbose=0, \n",
    "                        callbacks=[es])\n",
    "\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    plot_loss_accuracy(history, title=f'------ BATCH SIZE {batch_size} ------\\n The accuracy on the test set is of {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the oscillations of the accuracy and the loss with respect to the batch size number. Is this coherent with what we saw when playing with the Tensorflow Playground? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Questions** ❓ \n",
    "* How many optimizations of the weights are done within one epoch (with respect to the number of observations and the batch size)? \n",
    "* Therefore, is one epoch longer with a large or a small batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm455OX6ksyl"
   },
   "source": [
    "# Part IV: Regularization\n",
    "\n",
    "In this part of the notebook, we will see how to use `**regularizers**` in a neural network. \n",
    "\n",
    "Regularizers are used to `prevent overfitting` that can happen because very complex networks have many  parameters which tends to overfit the training data.\n",
    "\n",
    "First, let's initialize a model that has too many parameters for the task (many layers and/or many neurons) such that it overfits the training data  \n",
    "**To better see the effect, we will not use any early stopping criterion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "executionInfo": {
     "elapsed": 81596,
     "status": "ok",
     "timestamp": 1612905145614,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "XaOTe0-Yksyn"
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(25, activation='relu', input_dim=10))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,  validation_split=0.3,\n",
    "                    epochs=300, batch_size=16, verbose=0)\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'The accuracy on the test set is of {results[1]:.2f}')\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ZOjwm2ksyo"
   },
   "source": [
    "☝️ In our *overparametrized network*, some neurons became too specific to given training data, preventing the network from generalizing to new data. \n",
    "\n",
    "😕 This led to some overfitting. \n",
    "\n",
    "⚔️ We discovered the Early Stopping criterion as a weapon to fight overfitting.\n",
    "\n",
    "Two additional tools can be used to fight overfitting, they are specific layers:\n",
    "\n",
    "* ✂️ <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\">**`Dropout Layers`**</a> : \n",
    "    * Their role is to _cancel_ the output of some neurons  during the training part. \n",
    "    * By doing this at random, it prevents the network from getting too specific to the input data : no any neuron can be too specific to a given input as its output is sometimes cancelled by the Dropout Layer. Overall, it forces the information that is contained in one input sample to go through multiple neurons instead of only one specific neuron.\n",
    "\n",
    "* 👮🏻‍♀️ <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers\">**`Regularizers`**</a>: as Sequential Dense Neural Networks are simple activated linear regressions, the weights can be constrained using L1, L2 or L1-L2 penalties! Wow!\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Try to add dropout layers and regularization to all your layers of the above neural network and look at the effect on the loss on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 **Congratulations!** \n",
    "\n",
    "Don't forget to commit and push your challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}