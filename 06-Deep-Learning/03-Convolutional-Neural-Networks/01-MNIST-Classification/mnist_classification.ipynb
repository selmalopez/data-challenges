{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Day 3 - Mnist classification\n",
    "\n",
    "### Exercise objectives\n",
    "- Implement a CNN architecture with convolution layers\n",
    "- Run a Neural Network on images\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "Let's imagine for a moment that you are working for the postal office (and you're in 1970 / 1980). You deal everyday with a enourmous amont of letters, and you want to automate the process of reading the numbers that have been handwritten. This task, called the _Handwriting Recognition_, has been a very complex that has been handled by Bell Labs (among other) where Yann Le Cun used to work, and where such things have been developed : \n",
    "\n",
    "![Number recognition](recognition.gif)\n",
    "\n",
    "\n",
    "The idea is that you have an image (not a video: the animation is here to present what happens with different images) as an input and you try to predict the figure on the image - it corresponds to a classification task, where the output is the class (=figure) the image belongs to, from 0 to 9.\n",
    "\n",
    "This task used to be quite complex back in the time, and still is a benchmark on which a lot of people work. For this reason, the MNIST (for *Modified ou Mixed National Institute of Standards and Technology*) dataset has been created: it corresponds to digit images, from 0 to 9. \n",
    "\n",
    "You goal in this notebook is to build a Convolution Neural Network that can work on such images and predict the corresponding class of each digit image. Keep in mind that this CNN will make you classify hand-written digits, which was a very complex task till the 90's. \n",
    "\n",
    "## The data\n",
    "\n",
    "Keras provides multiple datasets within the Python package. You can load it with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ Let's look at some of the data. \n",
    "\n",
    "Select some of the values of the train set and plot them thanks to the `imshow` function from matplotlib with `cmap` set to `gray`(otherwise, the displayed colors are just some arrangement Matplotlib does, which does not exist in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[13], cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#classification\n",
    "plt.imshow(X_train[7], cmap=\"pink\");  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that neural networks converge faster when the input data are somehow normalized? It goes similarly for input images. \n",
    "\n",
    "❓ Question ❓ As a first preprocessing step, you should normalize your data. For images, it simply implies to divide your input data by the maximal value, i.e. 255. Don't forget to do it on your train and test data.\n",
    "\n",
    "(N.B.: you can also centered your data, by substracting 0.5 but it is not mandatory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#entre 0 et 1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ What is the shape of your images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the shape of my images are (10000, 28, 28)\n",
    "#6000 nombres totales d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that you have 60.000 training images, all of size (28, 28). However, Keras needs images whose last dimension is the number of channels, which is missing here.\n",
    "\n",
    "❓ Question ❓ Use the `expand_dims` to add one dimension at the end of the training and test data. Then, print the shape of X_train and X_test that should respectively be (60000, 28, 28, 1) and (10000, 28, 28, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a dimension of 1\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "X_train = expand_dims(X_train, axis=-1)\n",
    "X_test =expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 28, 28, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last thing to do to prepare your data is to convert your labels to one-hot encoded categories.\n",
    "\n",
    "❓ Question ❓ Use `to_categorical` to transform your labels. Store the results in `y_train_cat` and `y_test_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test) \n",
    "\n",
    "  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now ready to be used.\n",
    "\n",
    "## The Convolutional Neural Network _aka_ CNN\n",
    "\n",
    "Now, build a Convolutional Neural Network. \n",
    "\n",
    "❓ Question ❓ Based on the course, build a neural network that has:\n",
    "- a `Conv2D` layer with 8 filters, each of size (4, 4), with an input shape suitable for your task, the relu activation function, and padding='same' so as to \n",
    "- a `MaxPool2D` layer with a pool_size of (2, 2)\n",
    "- a second `Conv2D` layer with 16 filters, each of size (3, 3), and the relu activation function\n",
    "- a second `MaxPool2D` layer with a pool_size of (2, 2)\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the relu activation function\n",
    "- a last layer that is suited for your task\n",
    "\n",
    "In the function, do not forget to include the compilation of the model, which optimizes the `categorical_crossentropy` with the adam optimizer - and the accuracy should be among the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    ### First convolution & max-pooling\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), padding='same', activation=\"relu\")) #Conv2D\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))  #MaxPool\n",
    "    model.add(layers.Conv2D(16, (3,3), activation=\"relu\")) #Conv2D\n",
    "    \n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2))) #MaxPool2D\n",
    "    \n",
    "    model.add(layers.Flatten()) #Flatten\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='relu')) #dense\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model \n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ How many trainable parameters are there in your model?\n",
    "- Compute them with `model.summary()` first\n",
    "- Recompute them manually layer per layer then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5770      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a validation set and an early stopping criterion. \n",
    "- Limit at 5 epoch max in this challenge (just to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3544 - accuracy: 0.8903\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15b680700>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model = initialize_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          epochs=1,  # Use early stop in practice\n",
    "          batch_size=32, \n",
    "          verbose=1,callbacks=[es])\n",
    "\n",
    "\n",
    "# To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably see that the model converges within few epochs. The reason is that there are as many weight update as there are batches within each epoch. For instance, if you batch_size is of 32, you have 60.000/32 = 1875 updates.\n",
    "\n",
    "\n",
    "❓ Question ❓ What is your accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9627000093460083"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "accuracy[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should be already impressed by your skills! You solved what was a very hard problem 30 years ago with your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Congratulation! Don't forget to commit and push your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
