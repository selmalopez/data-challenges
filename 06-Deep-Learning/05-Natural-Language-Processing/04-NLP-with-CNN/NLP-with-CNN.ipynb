{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with CNN\n",
    "\n",
    "### Exercise objectives:\n",
    "\n",
    "- Use CNN instead of RNN for NLP\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "‚ö†Ô∏è **Warning** ‚ö†Ô∏è The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that a too large number of sentences will make your compute slow down, or even freeze - your RAM can even overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "‚ö†Ô∏è **DISCLAIMER** ‚ö†Ô∏è **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 15:54:01.144305: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-12 15:54:01.229658: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to do NLP, you need to go through one of the following option, as shown here : \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "But in both cases, you can replace the recurrent layer (top part) by a CNN layer. We will go into both, starting by the one on the left were the embedding is learnt in the network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Part 1 : Concatenate a Keras Embedding with a Conv1Düî• \n",
    "\n",
    "Let's train some fancy network here. \n",
    "\n",
    "Each of your word is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). So, all your sentences are actually represented as matrices once embedded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space. As shown here\n",
    "\n",
    "<img src=\"image_comparison.png\" width=\"500px\" />\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolution on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2 dimensional as the filters can move up and down, and left and right. In the case of our sentences, we want the corresponding kernel to move _only_ in the word by word direction (otherwise, moving coordinate of the embedding space by coordinate doesn't make much sense).\n",
    "\n",
    "So let's create a model that use convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the data\n",
    "\n",
    "‚ùì **Question** ‚ùì You will need to prepare your data. First, tokenize them. Then, you need to pad them (use a value `maxlen` equal to 150). You also might need to compute the size of your vocabulary ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30419 different words in your corpus\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#tokenize the data \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "### Let's write some mock-up data\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "print(f'There are {vocab_size} different words in your corpus')\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "### Pad your inputs\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post', maxlen=150)\n",
    "X_test_token_pad = pad_sequences(X_test_token, dtype='float32', padding='post',maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30419"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D Convolution.\n",
    "\n",
    "‚ùì **Question** ‚ùì Define a model that has :\n",
    "- an `Embedding` layer: `input_dim` is the `vocab_size + 1`, `output_dim` is the embedding space dimension, and `mask_zero` has to be set to `True`. Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a `Conv1D` layer \n",
    "- a `Flatten` layer\n",
    "- a `Dense` layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly\n",
    "\n",
    "‚ùó **Remark** ‚ùó The size of the `Conv1D` kernel corresponds exactly to the number of side-by-side words each kernel is taking into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 150, 100)          3042000   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 148, 20)           6020      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2960)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 5922      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3,053,945\n",
      "Trainable params: 3,053,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size to represent each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_length=150,\n",
    "    input_dim=vocab_size+1, # 30419 +1 for the 0 padding\n",
    "    output_dim=embedding_size,# 100\n",
    "    mask_zero=True, # Included masking layer \n",
    "))\n",
    "\n",
    "model.add(layers.Conv1D(20, kernel_size=3)) #Conv1D\n",
    "model.add(layers.Flatten()) #flatten \n",
    "model.add(layers.Dense(2, activation=\"relu\")) #dense\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at the number of parameters. You can compare it to the one that you had in previous exercise (esp. the first one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#3,042,205 params in this exercice\n",
    "#3,051,901 params in the first exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 150)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Fit your model with a stopping criterion, and evaluate it on the test data.\n",
    "\n",
    "You will probably notice that it is ... **much faster** than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 3s 42ms/step - loss: 0.6946 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.6064 - accuracy: 0.6486 - val_loss: 0.7043 - val_accuracy: 0.4787\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.3997 - accuracy: 0.9051 - val_loss: 0.7386 - val_accuracy: 0.5027\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.3220 - accuracy: 0.9914 - val_loss: 0.7613 - val_accuracy: 0.5293\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.2974 - accuracy: 0.9983 - val_loss: 0.7984 - val_accuracy: 0.5467\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.2811 - accuracy: 0.9994 - val_loss: 0.8464 - val_accuracy: 0.5427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1629697c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "\n",
    "#print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6916899681091309, 0.5163999795913696]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(X_test_token_pad, y_test, verbose=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Learn a Word2Vec representation, and then feed it to a NN with a `Conv1D`üî• \n",
    "\n",
    "In the first part of the exercise, you were asked to jointly learn the embedding representation and the CNN convolution within the same network, which was the CNN counterpart of the left part of this Figure : \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "Now, let's try to replace the RNN with a CNN for an architecture as on the right side.\n",
    "\n",
    "‚ùì **Question** ‚ùì Learn a word2vec model or load it directly from the one trained in GENSIM (transfer learning). Then, prepare your data as in the previous exercise. This question is quite long, it prepares you to real world challenges - but you have already done all the building bricks in the previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now construct a model that has a `Conv1D` layer, a flatten layer, a dense layer and an output layer, compile it and fit in on the train data. You can then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì You might been frustrated by the accuracy you got. This is where anyone gets at some point, when you have tested what came to your mind easily. Now, you need to improve your models, by making them more complex, changing the parameters, stacking additional layers, and so on.\n",
    "\n",
    "Only practice and tests will get you through it. So you can come back to your previous models, change them and try to get better results ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
