{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "We've just solved a binary classification problem. What about a **`multiclass classification task`**?\n",
    "\n",
    "### Exercise Objectives:\n",
    "- Write a Neural Network designed for a multiclass classification problem\n",
    "- Observe overfitting during the model's convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the data\n",
    "\n",
    "\n",
    "The **`make_blobs`** function [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) enables to draw : \n",
    "- an arbitrary number of data sample, argument `n_samples`\n",
    "- an arbitrary number of features per data sample, argument `n_features`\n",
    "- an arbitrary number of categories, argument `centers`\n",
    "- a distance between the categories, argument `cluster_std`\n",
    "\n",
    "There is also the `random_state` argument that allows to draw the data deterministically, in order to reproduce the same data. Two people who choose the same random_state will have the same data.\n",
    "\n",
    "‚ùì **Question** ‚ùì Based on the documentation, generate data with : \n",
    "- 1200 samples\n",
    "- 8 features per sample\n",
    "- 7 categories of data\n",
    "- 8 as the distance between the categories\n",
    "\n",
    "Select a `random_state` equal to 1.\n",
    "\n",
    "Print the shape and check that it corresponds to (1200, 8) for `X` and (1200,) for `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Using `matplotlib`, scatterplot two (arbitrary) dimensions of the input data together. Each dot should be colored with the category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Repeat the operation on other dimensions, to visualy that the data are not easily separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, the target `y` is a list of integers,  corresponding to the category of the input data.\n",
    "It looks like `[3, 2, 2, 3, 0, 5, 1, 1, 0, 5, ...]` (in this example, we have 7 categories, from 0 to 6).\n",
    "\n",
    "However, **`for categorical problems in Tensorflow.Keras, the target/output should be encoded`** in the following way:\n",
    "\n",
    "```\n",
    "[\n",
    "[0, 0, 0, 1, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0, 0], \n",
    "[1, 0, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 1, 0], \n",
    "[0, 1, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 1],\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "where:\n",
    "* the number of rows is equal to the number of observations\n",
    "* the number of columns is equal to the number of categories\n",
    "\n",
    "üëâ Each column corresponds to a category. \n",
    "\n",
    "üëâ Each row corresponds to a target, the 1 being the category the input data belongs to.\n",
    "\n",
    "You can view a row as a vector of probabilities.\n",
    "\n",
    "```\n",
    "Example:\n",
    "| Cat 0 | Cat 1 | Cat 2 | Cat 3 | Cat 4 | Cat 5 | Cat 6 |\n",
    "|-------|-------|-------|-------|-------|-------|-------|\n",
    "| 0     | 0     | 0     | 1     | 0     | 0     | 0     |\n",
    "\n",
    "means that for this given row, there is a 100% chance that the row belongs to the Cat 3\n",
    "```\n",
    "\n",
    "üí° To transform `y` to categories, use **`to_categorical`** function from Keras.\n",
    "\n",
    "*[Remark]* In a sense, *to_categorical* works a bit like the OneHotEncoder in Sklearn but instead of encoding a categorical feature, we are now encoding the target.\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì First print `y`, then apply the *`to_categorical`* to *`y`* and store the *`categorized version of y`* into **`y_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Split the dataset $X$ and $y_{cat}$  into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the data should always be standard-scaled, so as to lay _approximately_ in [-1, 1]. (We will see later why).\n",
    "\n",
    "‚ùì **Question** ‚ùì Fit a sklearn [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) on the train set and transform both your train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Complete the following function to initialize a model that has: \n",
    "- a first layer with 50 neurons, the `relu` activation and an appropriate input dimension\n",
    "- a output layer designed for a multiclassification task which outputs probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    ### Model architecture\n",
    "    pass  # YOUR CODE HERE\n",
    "    \n",
    "    ### Model optimization : Optimizer, loss and metric \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "### Note here that the loss is different! This is because the task is not with two categories only, therefore\n",
    "### the solver is somehow different (will see it tomorrow)\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì How many parameters (a.k.a. weights) are there in the model?\n",
    "\n",
    "How many parameters would a Deep Logistic Regression have with the same data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Fit your model on the train data with 50 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Evaluate your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Is this a good score? You should compare it to some sort of benchmark value. In this case, what score would a random guess give? Store this baseline score in the `accuracy_baseline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('baseline',\n",
    "                         accuracy=accuracy_baseline)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó Wait ... If you get a closer look at the plot of the loss, it seems that the loss was still decreasing after 50 epochs. Why stopping it so soon? Let's rerun the model (with the initialization first) with 1000 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì \n",
    "- What can you say about the new loss? \n",
    "- Evaluate once again your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó On one hand, the loss (computed on the train set) seems smaller than with 50 epochs. On the other hand, the accuracy on the test set got worse than before... \n",
    "\n",
    "‚ùì **Question** ‚ùì How is this phenomenon called? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó The overfitting occurs at some point during the iteration of the gradient descent, once the accuracy starts getting worse on the test set...\n",
    "\n",
    "‚ö†Ô∏è Therefore, we should:\n",
    "* either `choose a reasonable number of epochs to avoid overfitting` \n",
    "* or create what is called an `Early Stopping` criterion (cf. `Optimizers, Loss, Fitting` lecture tomorrow)\n",
    "\n",
    "Let's visually check when the test loss increases again in practice. \n",
    "\n",
    "üò± Yes, by using the test set, there is a massive data-leakage..., we should create a validation set for that in reality... again, cf. tomorrow's lecture!\n",
    "\n",
    "‚ùì **Question** ‚ùì Run the following command and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=500, \n",
    "                    batch_size=16,\n",
    "                    verbose=0)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Plot the values of the loss and accuracy on the train set (in blue) and on the test set (in orange). What can you comment on that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Reproduce similar results by defining a more complex architecture that includes : \n",
    "\n",
    "- a first layer with 25 neurons \n",
    "- a second layer with 15 neurons\n",
    "- a third layer with 10 neurons\n",
    "- a final layer that outputs probability for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "    pass  # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó \n",
    "- We clearly see that an overfitting can happend during the training. More in our next lecture\n",
    "- The model overfits as the number of parameters is very large (compare the number of weights with a logistic regression on the same data...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üèÅ Congratulations! Commit and push your notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}