{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-Up\n",
    "\n",
    "Start by running the usual Library Import cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load URLs from CSV\n",
    "\n",
    "If you paid attention to the files in your repo, you might have noticed a `urls.csv` in the project. Open it in VS Code, review the URLs and maybe add a few of your choice.\n",
    "\n",
    "Then load this CSV in a `urls_df` dataframe using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "urls_df = pd.read_csv('urls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich Dataset with an API\n",
    "\n",
    "Let's use the `fetch_metadata` function that we just implemented in the `opengraph.py` file.\n",
    "\n",
    "First let's import it and make sure that it works in the Notebook. \n",
    "\n",
    "1. Write the relevant `from ... import ...` line\n",
    "1. Call the `fetch_metadata` on a URL of your choice. You can write `fetch_` then `<TAB>` to autocomplete, then `<SHIFT> + <TAB>` to view the Docstring from your Python file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from opengraph import fetch_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the `urls_df` dataframe to add `title` and `description` columns for each URL\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ†˜ Hint</summary>\n",
    "\n",
    "  <p>Have a look at today's Lecture, you can start by copy/pasting what we did for <code>tracks_df</code> and adapt the code</p>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching for https://www.lewagon.com\n",
      "Fetching for https://stackoverflow.com/questions/tagged/python\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for index, row in urls_df.iterrows():\n",
    "    print(f\"Fetching for {row['url']}\")\n",
    "    metadata = fetch_metadata(row['url'])\n",
    "    urls_df.loc[index, 'title'] = metadata['title']\n",
    "    urls_df.loc[index, 'description'] = metadata['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your code!\n",
    "\n",
    "Run the cell below to check your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /Users/selmalopez/.pyenv/versions/lewagon_current/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/selmalopez/code/selmalopez/data-challenges/02-Data-Toolkit/02-Data-Sourcing/00-Warmup\n",
      "plugins: dash-2.0.0, anyio-3.3.2\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_warmup.py::TestWarmup::test_dataframe_has_new_columns \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.41s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/warmup.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed warmup step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('warmup',\n",
    "    df_columns=urls_df.columns,\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 6794ef67] Completed warmup step\n",
      " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
      " create mode 100644 02-Data-Toolkit/02-Data-Sourcing/00-Warmup/tests/warmup.pickle\n",
      "Enumerating objects: 12, done.\n",
      "Counting objects: 100% (12/12), done.\n",
      "Delta compression using up to 4 threads\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 788 bytes | 788.00 KiB/s, done.\n",
      "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
      "To github.com:selmalopez/data-challenges.git\n",
      "   561285ac..6794ef67  master -> master\n"
     ]
    }
   ],
   "source": [
    "git add tests/warmup.pickle\n",
    "\n",
    "git commit -m 'Completed warmup step'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Autoreload\n",
    "\n",
    "Today's Lecture introduced you to the usefulness of [`autoreload`](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) in the notebook, let's experiment with it!\n",
    "\n",
    "Run the following cell, it should return `True` if your method returns `None` when a website is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_metadata(\"https://www.a.com\") == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open VS Code and change the behavior of the function, to make it return an empty string `\"\"` rather than `None` if the HTTP response is something else than `200`. Save your file on the drive, and re-run the cell above.\n",
    "\n",
    "Do you see something changing? No? That's normal! The first version of the `fetch_metadata` code is stored in the Notebook Kernel.\n",
    "\n",
    "---\n",
    "\n",
    "OK, let's change back the `fetch_metadata` code in VS Code back to `None`.\n",
    "\n",
    "Then, add the following two lines to your first Notebook code cell:\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```\n",
    "\n",
    "Then in the menu bar, go to `Kernel` > `Restart & Run all`.\n",
    "\n",
    "---\n",
    "\n",
    "Now that autoreload is enabled, go to VS Code, and once again change the behavior so that it returns an empty string. Re-run the code cell above. Do you get `False`? Good! That means that the Notebook is now monitoring changes to the files imported, like `opengraph.py`, and will reload them if the code within them changes!\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "You might find this confusing, jumping through Notebook and VS Code, don't worry you will get used to it. The Notebook is a perfect tool to experiment, to keep notes, to get graphical output of the data, etc. Still, the end goal of a Data Team is to **ship** something (a product, an API, a model, etc.), so productizing the code and refactoring it _out_ of the Notebook into proper Python modules is a critical skill that you will learn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}